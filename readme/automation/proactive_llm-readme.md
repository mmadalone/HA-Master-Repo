# Proactive – Presence-Based Suggestions (Direct LLM)

![Proactive LLM](https://raw.githubusercontent.com/mmadalone/HA-Master-Repo/main/images/header/proactive_llm-header.jpeg)

A Home Assistant automation blueprint that proactively speaks AI-generated messages via TTS when presence is detected in a room during a configurable time window. Messages are generated on the fly by a Conversation / LLM agent using a prompt you configure, with optional repeated nagging while present and a bedtime yes/no question flow via Assist Satellite.

This is the **direct LLM** variant — a streamlined version that sends area and time-of-day context to the LLM without additional sensor injection. For the extended version with weekend overrides, context entity injection, media playing guards, and configurable user name pools, see `proactive_llm_sensors.yaml`.

## How It Works

```
Presence sensor ON (or nag tick)
         │
         ▼
┌──────────────────────────────┐
│  CONDITIONS (all must pass)   │
│  • Within active time window  │
│  • Presence still detected    │
│  • Repeat mode allows tick    │
│  • Cooldown elapsed           │
│  • Max nags not exceeded      │
└──────────────┬───────────────┘
               │ pass
               ▼
┌──────────────────────────────┐
│  BUILD CONTEXT                │
│  • Resolve time of day        │
│  • Area name + trigger type   │
│  • Select LLM prompt          │
└──────────────┬───────────────┘
               │
               ▼
┌──────────────────────────────┐
│  GENERATE MESSAGE             │
│  conversation.process →       │
│  LLM agent with prompt +     │
│  area + time context          │
│  (fallback on failure)        │
└──────────────┬───────────────┘
               │
               ▼
┌──────────────────────────────┐
│  SPEAK VIA TTS                │
│  Standard tts.speak or        │
│  ElevenLabs custom with       │
│  voice_profile                │
└──────────────┬───────────────┘
               │
               ▼ (if bedtime enabled)
┌──────────────────────────────┐
│  BEDTIME QUESTION             │
│  • Generate question via LLM  │
│  • ask_question on satellite  │
│  • YES → run bedtime script   │
│  • NO / timeout → do nothing  │
└──────────────────────────────┘
```

## Features

- **LLM-generated messages** — Each proactive message is unique, generated by your configured Conversation agent (OpenAI, Gemini, Ollama, local LLM, etc.) with area and time-of-day context.
- **Configurable cooldown** — Minimum time between messages. Acts as a nag interval when repeat mode is enabled.
- **Repeat / nag while present** — Optionally keeps speaking at the cooldown interval as long as presence stays on, with a configurable max nags per session limit.
- **ElevenLabs Custom TTS support** — Two TTS modes: standard `tts.speak` or ElevenLabs custom with `options.voice_profile`.
- **Bedtime question flow** — After the proactive message, optionally asks a yes/no bedtime question on an Assist Satellite using `assist_satellite.ask_question`. If you answer yes, a configurable bedtime script runs.
- **Robust fallbacks** — Time-of-day-aware static fallback messages if the LLM call fails or returns empty.
- **Cross-midnight time windows** — Active window can span midnight (e.g. 23:00 → 02:00).

## Comparison with `proactive_llm_sensors`

| Feature | This blueprint | `proactive_llm_sensors` |
|---------|---------------|------------------------|
| LLM message generation | ✅ | ✅ |
| Nag / cooldown / max sessions | ✅ | ✅ |
| ElevenLabs custom TTS | ✅ | ✅ |
| Bedtime question flow | ✅ | ✅ |
| Extra sensor context for LLM | — | ✅ |
| Weekend schedule overrides | — | ✅ |
| Weekend prompt / bedtime overrides | — | ✅ |
| Media playing guard | — | ✅ |
| Minimum presence duration | — | ✅ |
| Configurable user name pools | — | ✅ |
| Run-day selection | — | ✅ |
| `continue_on_error` on LLM calls | — | ✅ |

Use this blueprint when you want a simple per-area proactive speaker without the overhead of weekend profiles or sensor context. Use the sensors variant when you need the full feature set.

## Prerequisites

- A **Conversation agent** that supports `conversation.process` (OpenAI, Google Generative AI, Extended OpenAI, Ollama, local LLM, etc.)
- One or more **presence/occupancy sensors** (`binary_sensor`)
- A **media player** entity for TTS output
- A **TTS entity** (any HA-supported engine, or ElevenLabs custom via HACS)

**Optional:**
- An **Assist Satellite** entity for the bedtime question flow
- A **bedtime script** to run when the user answers yes

## Installation

1. Copy `proactive_llm.yaml` into your blueprints directory:
   ```
   config/blueprints/automation/<your_namespace>/proactive_llm.yaml
   ```
   Or import via URL if hosted on GitHub.

2. Create one automation per area: **Settings → Automations → Create Automation → Use Blueprint**

## Configuration

### ① Presence & Schedule

| Input | Default | Description |
|-------|---------|-------------|
| **Presence sensors** | — | One or more binary sensors indicating presence in this area |
| **Active from** | 08:00 | Start of the daily active window |
| **Active until** | 23:00 | End of the daily active window (supports crossing midnight) |

### ② Speaker & TTS

| Input | Default | Description |
|-------|---------|-------------|
| **Speaker / media player** | — | Media player to speak from in this area |
| **TTS mode** | standard_tts_entity | `standard_tts_entity` or `elevenlabs_custom_service` |
| **TTS entity** | — | TTS entity for `tts.speak` |
| **ElevenLabs voice profile** | — | Voice profile name/ID for ElevenLabs custom mode |

### ③ AI / LLM

| Input | Default | Description |
|-------|---------|-------------|
| **Conversation agent** | — | LLM agent for `conversation.process` |
| **Area name** | Workshop | Friendly name included in prompts and fallback messages |
| **LLM prompt** | (Quark personality) | Style/personality instructions for the proactive message |

### ④ Nagging Behavior

| Input | Default | Description |
|-------|---------|-------------|
| **Cooldown / nag interval** | 30 min | Minimum time between messages |
| **Keep nagging while present** | Off | Repeat at cooldown interval while presence stays on |
| **Max nags per session** | 3 | Limit per continuous presence session (0 = unlimited) |

### ⑤ Bedtime Feature

| Input | Default | Description |
|-------|---------|-------------|
| **Enable bedtime question** | Off | Ask a yes/no bedtime question after the proactive message |
| **Assist Satellite** | — | Satellite entity for `assist_satellite.ask_question` |
| **Bedtime LLM prompt** | (Quark bedtime personality) | Style instructions for the bedtime question |
| **Fallback bedtime text** | "Miquel, do you want me to help you go to bed now?" | Used if LLM fails |
| **Delay before question** | 5s | Wait after TTS before asking (lets audio finish) |
| **Bedtime script** | — | Script to run when user answers yes |

## Nag / Cooldown Logic

The automation uses two trigger types working together:

- **`presence_on`** (state trigger) — Fires when any presence sensor transitions OFF → ON.
- **Periodic tick** (`time_pattern`) — Fires every minute, filtered by conditions.

The cooldown check uses `this.attributes.last_triggered` to ensure the minimum interval. When `repeat_while_present` is enabled, periodic ticks are allowed through as long as presence is still on, cooldown has elapsed, and max nags haven't been exceeded.

**Session calculation:** Session start is the earliest `last_changed` among ON sensors. Max session duration = `cooldown × max_nags`. Once exceeded, nagging stops until presence resets.

## Technical Notes

- Runs in `mode: single` / `max_exceeded: silent` — prevents overlapping executions.
- Uses `condition: time` for schedule gating (supports cross-midnight windows natively).
- The `assist_satellite.ask_question` call uses `continue_on_error: true` to handle satellite failures gracefully.
- The time-of-day label is forced to "evening" whenever the bedtime question is enabled, ensuring the LLM generates contextually appropriate messages.
- LLM prompt context includes: area name, time-of-day label, trigger platform, and current timestamp.
- Requires **Home Assistant 2024.10.0** or newer.

## Relationship to Other Blueprints

This blueprint is the **v5 direct LLM** version in the proactive speaker lineage. It evolved into `proactive_llm_sensors.yaml` (v7), which adds weekend overrides, sensor context injection, user name pools, media guards, and per-day scheduling. Both share the same core architecture: presence trigger → conditions gate → LLM generation → TTS output → optional bedtime question.

## Author

**madalone**

## License

See repository for license details.

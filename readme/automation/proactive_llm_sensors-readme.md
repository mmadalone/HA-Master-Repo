# Proactive – Presence-Based Suggestions

![Proactive LLM Sensors](https://raw.githubusercontent.com/mmadalone/HA-Master-Repo/main/images/header/proactive_llm_sensors-header.jpeg)

A Home Assistant automation blueprint that proactively speaks AI-generated messages via TTS when presence is detected in a room during a configurable time window. Messages are generated on the fly by a Conversation / LLM agent using a prompt you configure, with optional repeated nagging, weekend schedule overrides, and a bedtime yes/no question flow via Assist Satellite.

Designed to be instanced per area — one for the workshop, one for the living room, one for the bedroom, each with their own schedule, personality prompt, and context sensors.

## How It Works

```
Presence sensor ON (or nag tick)
         │
         ▼
┌─────────────────────────────┐
│  CONDITIONS (all must pass)  │
│  • Allowed day of week       │
│  • Within active time window │
│  • Not speaking over media   │
│  • Min presence duration met │
│  • Still present in area     │
│  • Repeat mode allows tick   │
│  • Cooldown elapsed          │
│  • Max nags not exceeded     │
└──────────────┬──────────────┘
               │ pass
               ▼
┌─────────────────────────────┐
│  BUILD CONTEXT               │
│  • Pick random user name     │
│  • Resolve time of day       │
│  • Gather sensor states      │
│  • Select effective prompt   │
│    (weekday or weekend)      │
└──────────────┬──────────────┘
               │
               ▼
┌─────────────────────────────┐
│  GENERATE MESSAGE            │
│  conversation.process →      │
│  LLM agent with prompt +    │
│  area + time + context       │
│  (fallback on failure)       │
└──────────────┬──────────────┘
               │
               ▼
┌─────────────────────────────┐
│  SPEAK VIA TTS               │
│  Standard tts.speak or       │
│  ElevenLabs custom with      │
│  voice_profile               │
└──────────────┬──────────────┘
               │
               ▼ (if bedtime enabled)
┌─────────────────────────────┐
│  BEDTIME QUESTION            │
│  • Generate question via LLM │
│  • ask_question on satellite │
│  • YES → run bedtime script  │
│  • NO / timeout → do nothing │
└─────────────────────────────┘
```

## Features

- **LLM-generated messages** — Each proactive message is unique, generated by your configured Conversation agent (OpenAI, Gemini, Ollama, local LLM, etc.) with area, time-of-day, and sensor context baked into the prompt.
- **Sensor context injection** — Pass any entities (temperatures, UPS status, media players, lights, CPU temp) as live context to the LLM so it can comment on what's actually happening.
- **Configurable cooldown** — Minimum time between messages. Acts as a nag interval when repeat mode is enabled.
- **Repeat / nag while present** — Optionally keeps speaking at the cooldown interval as long as presence stays on, with a configurable max nags per session limit.
- **Weekend profile overrides** — Separate schedule, cooldown, and LLM prompt for weekend days. Can also disable the automation entirely on weekends.
- **ElevenLabs Custom TTS support** — Two TTS modes: standard `tts.speak` or ElevenLabs custom with `options.voice_profile`.
- **Random name variety** — Picks a random name/nickname from a configurable list each run, with separate fallback pools for direct speech (TTS) and LLM prompt references (3rd person).
- **Media playing guard** — Optionally silences itself when media is actively playing on the target speaker.
- **Minimum presence duration** — Prevents quick walk-by triggers from firing.
- **Bedtime question flow** — After the proactive message, optionally asks a yes/no bedtime question on an Assist Satellite using `assist_satellite.ask_question`. If you answer yes, a configurable bedtime script runs. Weekend-specific bedtime prompt overrides available.
- **Robust fallbacks** — Static fallback messages for both proactive and bedtime text if the LLM call fails.

## Prerequisites

- A **Conversation agent** that supports `conversation.process` (OpenAI, Google Generative AI, Extended OpenAI, Ollama, local LLM, etc.)
- One or more **presence/occupancy sensors** (`binary_sensor` with `device_class: occupancy`)
- A **media player** entity for TTS output
- A **TTS entity** (any HA-supported engine, or ElevenLabs custom via HACS)

**Optional:**
- An **Assist Satellite** entity for the bedtime question flow
- A **bedtime script** to run when the user answers yes
- **Context entities** — any sensors/entities to feed as live context to the LLM

## Installation

1. Copy `proactive_llm_sensors.yaml` into your blueprints directory:
   ```
   config/blueprints/automation/<your_namespace>/proactive_llm_sensors.yaml
   ```
   Or import via URL if hosted on GitHub.

2. Create one automation per area: **Settings → Automations → Create Automation → Use Blueprint**

## Configuration

### ① Presence & Detection

| Input | Default | Description |
|-------|---------|-------------|
| **Presence sensors** | — | One or more occupancy sensors for this area. Fires when ANY transitions to ON. |
| **Minimum presence duration** | 0s | Seconds presence must be continuous before speaking. Prevents walk-by triggers. |
| **Block if media playing** | Off | Skip speaking while the target media player is actively playing. |

### ② TTS & Speaker

| Input | Default | Description |
|-------|---------|-------------|
| **Speaker / media player** | — | Media player to speak from in this area. |
| **TTS mode** | Standard | `standard_tts_entity` or `elevenlabs_custom_service` |
| **TTS entity** | — | TTS entity for `tts.speak` |
| **ElevenLabs voice profile** | — | Voice profile for ElevenLabs custom mode |
| **Area name** | Workshop | Friendly name included in prompts and fallback messages |

### ③ AI Conversation

| Input | Default | Description |
|-------|---------|-------------|
| **User names** | — | Comma-separated names/nicknames. One picked randomly per run. |
| **Fallback names** | "friend, hey there" | Direct-speech fallback terms when no user name is set. |
| **LLM fallback names** | "the user" | 3rd-person fallback terms for LLM prompt instructions. |
| **Conversation agent** | — | LLM agent for `conversation.process` |
| **LLM prompt** | (playful assistant) | Style/personality instructions for the proactive message |
| **Context entities** | — | Extra sensors/entities passed as live context to the LLM |

### ④ Schedule & Timing

| Input | Default | Description |
|-------|---------|-------------|
| **Active from / until** | 08:00–23:00 | Time window (supports crossing midnight) |
| **Run on these days** | All days | Weekday selection |
| **Cooldown / nag interval** | 30 min | Minimum time between messages |
| **Keep nagging while present** | Off | Repeat at cooldown interval while presence stays on |
| **Max nags per session** | 3 | Limit per continuous presence session (0 = unlimited) |

### ⑤ Weekend Overrides

| Input | Default | Description |
|-------|---------|-------------|
| **Weekend behavior** | Same as weekdays | `same_as_weekdays`, `disabled_on_weekends`, or `use_weekend_profile` |
| **Weekend days** | Sat, Sun | Which days count as weekend |
| **Weekend active from / until** | 08:00–23:00 | Weekend-specific time window |
| **Weekend cooldown** | 30 min | Weekend-specific cooldown |
| **Weekend LLM prompt override** | — | Alternate prompt for weekends (blank = use normal prompt) |

### ⑥ Bedtime Question

| Input | Default | Description |
|-------|---------|-------------|
| **Enable bedtime question** | Off | Ask a yes/no bedtime question after the proactive message |
| **Assist Satellite** | — | Satellite entity for `assist_satellite.ask_question` |
| **Delay before question** | 5s | Wait after TTS before asking (lets audio finish) |
| **Bedtime LLM prompt** | (bedtime assistant) | Style instructions for the bedtime question |
| **Fallback bedtime text** | "Do you want me to help you go to bed now?" | Used if LLM fails |
| **Bedtime script** | — | Script to run when user answers yes |
| **Weekend bedtime mode** | Same as weekdays | `same_as_weekdays`, `disabled`, or `use_weekend_bedtime_prompt` |
| **Weekend bedtime prompt override** | — | Alternate bedtime prompt for weekends |

## Nag / Cooldown Logic

The automation uses two trigger types working together:

- **`presence_on`** — Fires when any presence sensor transitions OFF → ON (initial entry).
- **`nag_tick`** — A `time_pattern` trigger firing every minute, filtered by conditions.

The cooldown check uses `this.attributes.last_triggered` to ensure the minimum interval between messages. When `repeat_while_present` is enabled, `nag_tick` triggers are allowed through as long as presence is still on, cooldown has elapsed, and max nags haven't been exceeded.

**Session calculation:** Session start is the earliest `last_changed` among ON sensors. Max session duration = `cooldown × max_nags`. Once exceeded, nagging stops until presence resets (all sensors go OFF, then one comes back ON).

## Technical Notes

- Runs in `mode: single` / `max_exceeded: silent` — prevents overlapping executions and double-speaking.
- Stores 15 traces for debugging (`trace.stored_traces: 15`).
- Time window supports crossing midnight (e.g. 23:00 → 02:00).
- All LLM calls use `continue_on_error: true` with fallback message extraction.
- The `assist_satellite.ask_question` service has a built-in timeout; `continue_on_error` covers integration failures.
- Requires **Home Assistant 2024.10.0** or newer.

## Author

**madalone**

## License

See repository for license details.
